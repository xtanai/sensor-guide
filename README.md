# üé• Recommended Sensor Options

for 3D Hand-Tracking Reconstruction

**Short note ‚Äî not recommended:** Standard **RGB webcams**, **Meta Quest 3**, **iPhone 15 Pro Max**, etc. (rolling shutter, auto exposure/gain, weak IR response) are fine for experiments but **unsuitable for precise hand tracking**. Likewise, **depth solutions** (LiDAR/ToF/active stereo ‚Äî e.g., Intel RealSense, Kinect) are often **ill-suited** to fine hand/finger work: **relatively expensive**, **lower/variable FPS**, and **insufficient precision** for **parametric CAD commands**.

---

## üì∑ Key sensor parameters

* **Type**

  * ‚úÖ **Monochrome sensors (preferred)** ‚Äì maximum sharpness, no Bayer debayering, better NIR sensitivity.
  * ‚ùå **RGB color** ‚Äì for debugging/visualization only; not recommended for precise 3D reconstruction.
  * *Note:* An **RGB camera placed between two mono cameras** can still be useful as an auxiliary channel for **calibration support** and **texture/context cues** (without driving the core 3D reconstruction).

* **Shutter**

  * ‚úÖ **Global shutter** ‚Äì required for precise 3D, fast motion, and synchronized multi-view rigs.
  * ‚ùå **Rolling shutter** ‚Äì causes distortions with motion and unsynchronized exposures.

* **Sync / Trigger I/O**

  * ‚úÖ **Required.** The sensor **must** support external sync/trigger (input/output), driven by our MCU / TDM controller.
  * ‚ùå **No trigger support** ‚Äì not acceptable for this project.
  * Goal: **frame-accurate synchronization** of all cameras in the rig.
  * *Nice to have:* a STROBE / exposure-active output that can be used as a hardware feedback signal (trigger accepted / exposure occurred).

* **Pixel format**

  * ‚úÖ **RAW10** or **RAW12** ‚Äì preferred (see rating table above).
  * ‚ùå **MJPEG** or full **YUV color formats** ‚Äì only for preview/debug, **not** for the main 3D/measurement pipeline.
  * **RAW12** offers better quantization (more effective intensity/depth levels), at the cost of higher bandwidth and compute.
  * **Fallback:** **RAW8 / Y8** is acceptable for low-cost / low-bandwidth prototypes.
 
* **Spectral / optics**

  * ‚úÖ **CMOS without IR-cut filter** ‚Äì full NIR sensitivity.
  * ‚ùå **IR-cut filter** ‚Äì not recommended; it blocks most NIR light and severely reduces signal at 850 
  * Use an **850 nm band-pass filter** with matching NIR illumination (LED/VCSEL).
  * Goal: stable contrast in NIR, robust against visible-light textures, colors, and ambient light.
  * Optional: If **specular reflections** become an issue, consider using a **polarizer** (only helps in certain geometries and may reduce overall signal).

* **Sensor size**

  * üéØ Preferred: **1/4"** ‚Äì good trade-off between cost, size, and lens availability.
  * Optional: **~1/2.5"** ‚Äì more light, better SNR, more optical flexibility, but higher cost and larger form factor.

* **Frame rate**

  * üéØ Target: **‚â• 120 FPS** (for fast gestures, low latency, and better temporal fusion).
  * Acceptable for early development: **‚â• 60 FPS**.

* **Resolution**

  * üéØ Minimum: **‚â• 1280 √ó 800**.
  * Higher resolutions (**‚â• 2 MP**) only if:

    * your full pipeline can handle **RAW10/RAW12** at higher bandwidth, and
    * the additional compute load (stereo matching / pose estimation) is planned for.

* **Interface**

  * üéØ **Lowest latency / best determinism: MIPI-CSI**

  * Alternatives (valid, but not my primary use case):

    * **USB3 Vision**
      ‚Üí Good for single industrial cameras; cable length is limited without active extenders/repeaters.
    * **GigE Vision (‚â• 2.5 GbE)**
      ‚Üí Robust over longer distances; PoE possible; scales well for distributed/industrial rigs.
    * **SFP+**
      ‚Üí Great for very high bandwidth and long runs, but typically requires custom hardware and is rarely fully off-the-shelf.
    * **CoaXPress**
      ‚Üí Top-tier on bandwidth, latency, and cable length, but significantly more expensive and complex to integrate.

* **Field of View (FOV)**

  * üéØ Target range: **60‚Äì120¬∞**, depending on:

    * rig geometry,
    * working distance,
    * desired measurement volume (e.g., hand vs. full upper body).
  * For detailed formulas and examples, see: **[Vision Geometry Rules](https://github.com/xtanai/geo_rules)**.

---

## ‚≠ê Pixel format ‚Äì preference

| Format                     |   Rating   | Comment                                                                                  |
| -------------------------- | ---------- | ---------------------------------------------------------------------------------------- |
| **MJPEG / JPEG**           | ‚≠ê‚òÜ‚òÜ‚òÜ‚òÜ     | Only for preview/debug. Strong artifacts, variable bitrate, poor for precise 3D.         |
| **YUV / YUYV / NV12**      | ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ    | OK if you only use the **Y (luma)** channel. Extra bandwidth wasted on color info.       |
| **RAW8 / Y8 (8-bit mono)** | ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ   | Solid baseline. Lower dynamic range, but good enough with proper NIR illumination.       |
| **RAW10**                  | ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ  | Very good: higher dynamic range, finer quantization, still manageable bandwidth.         |
| **RAW12**                  | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Ideal for high precision: maximum dynamic range and depth resolution, highest bandwidth. |

---

## ‚öôÔ∏è Quick Engineering Comparison ‚Äî What is the best interface for deterministic vision?

When designing a machine-vision or stereo system, the choice of sensor interface has a strong impact on latency, control, and system complexity.
Below is a simplified engineering comparison:

| Interface              | Additional Chips / Infra | RAW Access | Latency     | Determinism |
| ---------------------- | ------------------------ | ---------- | ----------- | ---------- |
| **MIPI CSI-2**         | very few                 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **USB2 (typical UVC)** | medium                   | ‚≠ê‚òÜ‚òÜ‚òÜ‚òÜ     | ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ     | ‚≠ê‚òÜ‚òÜ‚òÜ‚òÜ     |
| **USB3 (typical UVC)** | medium                   | ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ    | ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ   | ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ     |
| **GigE / GigE Vision** | many                     | ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ  | ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ   | ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ  |
| **CoaXPress**          | heavy (framegrabber)     | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

### Summary

**MIPI CSI-2** is typically the best choice when deterministic timing, minimal latency, and direct RAW sensor access are required. The sensor is connected almost directly to the SoC, which reduces hidden processing stages and keeps the pipeline transparent.

**USB cameras** usually include additional ISP and bridge chips. They are convenient and plug-and-play, but often introduce internal processing and buffering that reduce determinism.

**GigE cameras** are powerful for industrial networking and long cable distances, but typically require more intermediate logic (FPGA/ASIC, packetization, buffering), which increases system complexity.

**CoaXPress** is a high-end industrial interface designed for very high bandwidth and deterministic transmission. It typically requires a dedicated frame grabber card on the host side and specialized hardware inside the camera. While it offers excellent throughput, low latency, and strong determinism, it significantly increases system cost, hardware complexity, and power requirements compared to embedded MIPI-based designs.

For edge-processing architectures focused on precise timing and reproducible results, **MIPI CSI-2 provides the most transparent and controllable capture path**.

---

## üìç Recommended sensors (available modules)

Below are practical, readily available **global-shutter** camera modules that work well for NIR / tracking / stereo on Raspberry Pi (CSI-2 MIPI).

> **Lens note:** Very wide lenses (e.g. **130¬∞**) are usually **not recommended** for stereo/tracking because they reduce per-pixel energy (darker image), increase distortion, and make illumination harder.  
> For most setups, **~70‚Äì80¬∞ FoV** is a good starting point (depending on working distance and baseline).

### ‚ö° Up to ~120 FPS (1 MP class)

**OV9281 (1 MP) mono global shutter**
- MyBotShop (ArduCAM OV9281, 130¬∞ lens)  
  [Link](https://www.mybotshop.de/ArduCAM-1MP-OV9281-Mono-Global-Shutter-130deg-M12)
  **Note:** The included **130¬∞ lens is not recommended** ‚Üí swap to **~70‚Äì80¬∞** FoV if possible.
- Inno-Maker OV9281 module  
   [Link](https://www.inno-maker.com/product/cam-mipiov9281/)
- Inno-Maker OV9281 RAW v2  
   [Link](https://www.inno-maker.com/product/cam-mipi9281raw-v2/)

**Why OV9281?**
- Very common in stereo / tracking projects
- Good mode support and high frame rates (depending on ROI / sensor modes)
- Mono + no IR-cut options available on many modules

### üéûÔ∏è Up to ~60 FPS (higher resolution)

**OV2311 (2 MP) mono global shutter**
- ArduCAM OV2311 for Raspberry Pi  
  [Link](https://www.arducam.com/arducam-ov2311-mipi-2mp-mono-global-shutter-camera-module-for-raspberry-pi.html)

**IMX296 (1.58 MP) global shutter**
- ArduCAM IMX296 module  
  [Link](https://www.arducam.com/1-58mp-imx296-mono-global-shutter-camera-module-with-m12-lens-for-raspberry-pi.html)
- Inno-Maker IMX296 RAW + trigger variant  
  [Link](https://www.inno-maker.com/product/cam-mipi296raw-trigger/)
  
**Why these?**
- Higher resolution compared to OV9281
- Larger optical format than typical 1/4" sensors (better light collection per frame)
- Useful when you need more spatial detail and can accept a lower maximum frame rate

### üß© Other / stereo kits (caution)

**ArduCAM dual OV9281 wide-angle stereo kit**
-  [Link](https://www.arducam.com/arducam-1mp2-wide-angle-stereo-camera-for-raspberry-pi-jetson-nano-and-xavier-nx-dual-ov9281-monochrome-global-shutter-camera-module.html)

‚ö†Ô∏è **Note:** This kit is often **not ideal** for synchronized IR strobe / trigger use-cases, because it does **not provide a proper external trigger/strobe interface** (depends on variant).  
For time-multiplexed strobing (TDM) you typically want modules with **trigger input** or a well-defined sync mechanism.

### üí° General advice
- Prefer **mono + no IR-cut** for 850 nm NIR illumination.
- Prefer camera modules with **trigger/sync support** if you plan IR strobes (especially TDM).
- Plan lens FoV together with your illumination: **wide FoV = more LED power needed**.


---

## üß≠ Mono Sensors on the Market (MIPI CSI-2)

For **mono global-shutter stereo with hardware trigger** on Raspberry Pi (MIPI CSI-2), there are only **a few** affordable and widely available options. The most accessible and practical modules today are typically based on **OV9281**, **Sony IMX296LLR**, and **onsemi AR0145**. These sensors are common in the Raspberry Pi ecosystem and work well for early development, prototyping, and validation.


### Current Prototype

The current prototype uses OV9281-, IMX296LLR, AR0145-based modules, currently configured with a **limited CSI-2 lane setup**. This is ideal for development and validation, but it does **not fully utilize** the Raspberry Pi 5‚Äôs available CSI bandwidth and leaves less headroom for higher-throughput capture modes.

### Long-Term Goal: Higher-Bandwidth RAW Sensors (Transparent Capture Path)

For the next generation, I‚Äôm targeting a **RAW-first sensor path** with **transparent, deterministic control**‚Äîmeaning no opaque or mandatory processing stages in the capture chain. The goal is to take full advantage of **4-lane CSI-2 on Raspberry Pi 5**, increasing throughput and leaving more headroom for advanced modes and future upgrades.

Today‚Äôs sensors and modes are still bounded by the Raspberry Pi 5 CPU budget, so the current design stays realistic and avoids ‚Äúoverbuilding‚Äù features that the Pi 5 cannot efficiently run. At the same time, the pipeline is structured to be future-ready: as soon as a faster platform (e.g., a future Pi 6) becomes available, the same RAW-first architecture can scale to higher-end sensors and more demanding modes without needing a major redesign.

Securing the right sensor/module partner is feasible‚Äîbut it depends on **validated demand**.

### Open, Repeatable Platform (MotionCoder-first)

This is not meant to be closed or exclusive. The platform should remain **repeatable and widely adoptable**, because the long-term value sits in the software layer‚Äî**MotionCoder**‚Äîwhich benefits most from a broadly available, standardized capture backend.

### Feasibility & Manufacturing Reality

From a system perspective, this direction is realistic: a RAW-first camera path can keep the electronics BOM lean, while Linux-based edge platforms provide strong, direct control over capture, timing, and the imaging pipeline.

The main challenge is not ‚Äúextra chips‚Äù - it‚Äôs **reliable module sourcing**, **mechanical integration**, **QC**, and **calibration**. That‚Äôs exactly what demand validation helps unlock.

Based on current assumptions, I estimate the sensor/module cost can be kept below **~‚Ç¨100 per unit**, but this depends heavily on volume. In many cases, a **MIPI CSI-2** camera path can be **significantly cheaper than typical USB webcams**, because the electronics BOM is often leaner (fewer bridge/ISP components and a more direct sensor-to-SoC connection). Achieving stable pricing and supply, however, usually requires a meaningful production run (on the order of **~1,000 units**).

### Vote / Demand Signal (Call to Action)

If you want **higher-end mono sensors** (more bandwidth, better optics support, stronger long-term supply), please leave a **vote / comment**. I‚Äôm collecting demand signals to move forward with supplier negotiations, final sourcing, and production commitments.

Once demand is validated, I can confidently push this to production‚Äîso you can **swap camera modules in EdgeTrack** and keep using the same high-end capture backend as sensors evolve.

---




